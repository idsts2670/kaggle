{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 必要なライブラリのimport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "\n",
    "# matplotlibの日本語化対応\n",
    "import japanize_matplotlib\n",
    "\n",
    "# データフレーム表示用関数\n",
    "from IPython.display import display\n",
    "\n",
    "# 表示オプション調整\n",
    "# numpyの浮動小数点の表示精度\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# pandasでの浮動小数点の表示精度\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# データフレームですべての項目を表示\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "# グラフのデフォルトフォント指定\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "# グラフの表示方法調整\n",
    "sns.set(rc={'figure.figsize':(12,5)});\n",
    "plt.figure(figsize=(12,5));\n",
    "\n",
    "# 乱数の種\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイマー\n",
    "# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは\n",
    "機械学習とは、ざっと言ってしまうとあるデータ X を入力として対応する予測値 y を取り出すような対応関係を作成することです。\n",
    "\n",
    "例：タイタニック号で、乗客が生きるか死ぬかを予測する問題だと X は乗客の年齢, 性別, 船室のグレード… など乗客に紐づく情報のことを指します。通常、この情報のことを特徴量とよびます。\n",
    "\n",
    "特徴量 X と 予測値 y が用意できれば学習用データ (X - y の関係がわかっているデータ) を元にして X をいれて y になるようにモデルを調整する。この調整の段階を学習とよびます。学習には様々なアルゴリズムがあるが、X, y を用意しなくてはならない部分は基本的に変わらないです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回のコンペに関して"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンペ自体のデータサイズが大きいのでデータハンドリングが僕たちに取っては肝になりそうです\n",
    "\n",
    "> データの読み込みに関して\n",
    "* 形式を変えたデータセットをpandasで読み込む\n",
    "  * [`feather`](https://www.kaggle.com/datasets/munumbutt/amexfeather) or [`Parquet`](https://www.kaggle.com/datasets/odins0n/amex-parquet)\n",
    "* [`pyspark`](https://www.kaggle.com/code/rakkaalhazimi/export-large-dataset-to-spark) or [`dask`](https://docs.dask.org/en/latest/dataframe.html) で読み込む\n",
    "\n",
    "> サイズが大きい場合の対処方法は以下のkaggle notebookを参考にしてください\n",
    "\n",
    "[How to Work with BIG Datasets on 16G RAM (+Dask)](https://www.kaggle.com/code/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask)\n",
    "\n",
    "以下がnotebookの内容概要\n",
    "* TIP 1 - 使用していない変数を [`delete`](https://www.sejuku.net/blog/74447) するか & gc.collect()` で[ガベージコレクション](https://techacademy.jp/magazine/19437)(不要になったメモリ領域を開放して再利用する機能)をする\n",
    "* TIP 2 - データセット内の各カラムのデータタイプを予め定義しておく\n",
    "  * eg: 本来 float64 だったものを -> float32 と定義してサイズを圧縮する\n",
    "* TIP 3 - 読み込むデータセットを選択する (including generating your own subsamples)\n",
    "  * 読み込む行数を選ぶ\n",
    "  * (`skiprows`) で[読み飛ばす行数を指定する](https://bit.ly/3O90Ze7)\n",
    "  * 読み飛ばすリストを作成して、読み飛ばす（以下例）\n",
    "    * ```\n",
    "      skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)\n",
    "      skiplines=np.sort(skiplines)\n",
    "      train = pd.read_csv('../input/train.csv', skiprows=skiplines, dtype=dtypes)\n",
    "      ```\n",
    "* TIP 4 - バッチ処理をする\n",
    "  * ひとまとまりのデータに対して、一連の処理を連続で実行する処理方式のこと。大きなデータに関しても、設定したデータ数(チャンク)ごとに処理をする\n",
    "* TIP 5 - 特定のカラムのみ `import` する\n",
    "  * 450,000行 × 150カラム より 100万行 × 2カラム の方がメモリ消費が少ないことは容易に想像できる\n",
    "* TIP 6 - groupby などの処理をするときも一部カラムの一部データのみにするˆ\n",
    "* TIP 7 - `Dask` を使用する。\n",
    "  * [DaskについてのQiita記事](https://qiita.com/simonritchie/items/e174f243bc03fb25462e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンペの概要\n",
    "* コンペ名：[American Express - Default Prediction](https://www.kaggle.com/competitions/amex-default-prediction)\n",
    "\n",
    "* 目的：毎月の顧客プロファイルから、顧客がクレジットカードの残高分を将来返済しない確率を予測すること\n",
    "  * ターゲットのバイナリ変数は、最新のクレジットカード明細書から18ヶ月間のパフォーマンスウィンドウを観察することによって計算され、顧客が最新の明細書の日付から120日以内に支払額を支払わない場合、デフォルトとみなされる。\n",
    "\n",
    "* 評価方法(Evaluation)\n",
    "  * このコンペではクレカのデフォルト率を予測する。サブミットはちょっと特殊な評価方法で評価される。以下の通り:\n",
    "    * ```\n",
    "      M = 0.5*(G+D)  (*G = 正規化ジニ係数, D = デフォルト率 4% )\n",
    "      ```\n",
    "      機械学習における `正規化ジニ係数` は経済学などで使用される ジニ係数とは違うので注意。以下記事参照\n",
    "      * [機械学習のモデル評価、説明可能性のための指標　その１。ジニ係数とAUC](https://qiita.com/Derek/items/4ded249f7a75f8da176c)\n",
    "      * [DataRobot](https://docs.datarobot.com/ja/docs/modeling/reference/model-detail/opt-metric.html#gini-coefficient)\n",
    "      * [GINI and AUC relationship](https://stats.stackexchange.com/questions/342329/gini-and-auc-relationship)\n",
    "      * [Why use Normalized Gini Score instead of AUC as evaluation?](https://stats.stackexchange.com/questions/306287/why-use-normalized-gini-score-instead-of-auc-as-evaluation)\n",
    "\n",
    "* データについて\n",
    "  * データセットには各顧客の特徴が各明細書日付ごとに集約されたものが含まれている。特徴は匿名化され、正規化されており、以下のカテゴリに分類されている（カラムの prefix を見ると分かる）:\n",
    "    * D_*: Delinquency variables\n",
    "    * S_*: Spend variables\n",
    "    * P_*: Payment variables\n",
    "    * B_*: Balance variables\n",
    "    * R_*: Risk variables\n",
    "  * 以下のカラムはカテゴリ変数である:\n",
    "    * B_30, B_38, D_63, D_64, D_66, D_68, D_114, D_116, D_117, D_120, D_126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ管理環境整備\n",
    "\n",
    "≒ディレクトリ整備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_dir（input directory） を作る\n",
    "current_note_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "INPUT_DIR = os.path.join(current_note_path, \"data\")\n",
    "\n",
    "# INPUT_DIRがまだ作られていなければ作成\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    os.mkdir(INPUT_DIR)\n",
    "\n",
    "# output_dir(output directory) を作る\n",
    "OUTPUT_DIR = os.path.join(current_note_path, 'outputs')\n",
    "\n",
    "# OUTPUT_DIRがまだ作られていなければ作成\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/test_data.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/train_labels.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/train_data.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 一度やったらコメントアウトする\n",
    "\n",
    "# # csvファイルを `data` ディレクトリ（=フォルダー） に移動させる\n",
    "# unique_dir_names = []\n",
    "# for f in Path(f'{current_note_path}').rglob('*.csv'):\n",
    "#     unique_dir_names.append(f)\n",
    "\n",
    "# for file in list(set(unique_dir_names)):\n",
    "#     print(f'moved file: {file}')\n",
    "#     shutil.move(f'{file}', f'{INPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv(or any file)を簡単に読み込めるようにする関数\n",
    "# csv を読み取る関数を設定したあげると、pathや拡張子を書かずに読み込めるので入力が楽になる\n",
    "\n",
    "# 通常のcsvファイルを読み取るときは以下\n",
    "def read_csv(name, **kwrgs):\n",
    "    path = os.path.join(INPUT_DIR, name + '.csv')\n",
    "    print(f'Load: {path}')\n",
    "    return pd.read_csv(path, **kwrgs)\n",
    "\n",
    "# 今回はparquet形式を読み込むので別途用意\n",
    "def read_parquet(name, **kwrgs):\n",
    "    path = os.path.join(INPUT_DIR, name + '.parquet')\n",
    "    print(f'Load: {path}')\n",
    "    return pd.read_parquet(path, **kwrgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前記の通り、データサイズが大きいため、pandas などを安易に使ってもローカルではメモリが足りないので、ここでは2つの方法を取る\n",
    "* parquet 形式のデータを読み込む (*本来は自分でcsv -> parquet形式に変える必要があるが、心優しい人がparquet形式にしてくれているのでそれを参照する。)\n",
    "  * [該当データダウンロードページ](https://www.kaggle.com/datasets/odins0n/amex-parquet)\n",
    "  * [Load Parquet Files with Low Memory](https://www.kaggle.com/code/odins0n/load-parquet-files-with-low-memory)\n",
    "  * [parquetデータを使用したEDA](https://www.kaggle.com/code/odins0n/amex-default-prediction-detailed-eda)\n",
    "* a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/data/test_data.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000009?line=0'>1</a>\u001b[0m test_df \u001b[39m=\u001b[39m read_parquet(\u001b[39m'\u001b[39;49m\u001b[39mtest_data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39m#train_df = read_parquet('train_data')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000009?line=2'>3</a>\u001b[0m train_labels \u001b[39m=\u001b[39m read_csv(\u001b[39m'\u001b[39m\u001b[39mtrain_labels\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb Cell 10'\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(name, **kwrgs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000007?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoad: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000007?line=13'>14</a>\u001b[0m \u001b[39m#return pd.read_parquet(path, **kwrgs)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/amex-default-prediction.ipynb#ch0000007?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mread_table(path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwrgs)\n",
      "File \u001b[0;32m~/miniforge3/envs/python38/lib/python3.8/site-packages/pyarrow/parquet.py:1941\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m   1933\u001b[0m         \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[1;32m   1934\u001b[0m         dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[1;32m   1935\u001b[0m             source, metadata\u001b[39m=\u001b[39mmetadata, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[1;32m   1936\u001b[0m             memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[1;32m   1937\u001b[0m             pre_buffer\u001b[39m=\u001b[39mpre_buffer,\n\u001b[1;32m   1938\u001b[0m             coerce_int96_timestamp_unit\u001b[39m=\u001b[39mcoerce_int96_timestamp_unit\n\u001b[1;32m   1939\u001b[0m         )\n\u001b[0;32m-> 1941\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[1;32m   1942\u001b[0m                         use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n\u001b[1;32m   1944\u001b[0m \u001b[39mif\u001b[39;00m ignore_prefixes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1946\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore_prefixes\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is only supported when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1947\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muse_legacy_dataset=False\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/python38/lib/python3.8/site-packages/pyarrow/parquet.py:1776\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[39mif\u001b[39;00m use_threads:\n\u001b[1;32m   1772\u001b[0m         \u001b[39m# Allow per-column parallelism; would otherwise cause\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m         \u001b[39m# contention in the presence of per-file parallelism.\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m         use_threads \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[1;32m   1777\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[1;32m   1778\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[1;32m   1779\u001b[0m )\n\u001b[1;32m   1781\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test_df = read_parquet('test_data')\n",
    "train_df = read_parquet('train_data')\n",
    "train_labels = read_csv('train_labels')\n",
    "sample_sub = read_csv('sample_submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ガベージコレクション\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: object(4), float64(185), int64(1)"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のカラムは頻出で、毎回入力するのはめんどくさいので、ポップアップされるように定義します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各テーブルの概説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (データ確認)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値・異常値確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理 + 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリ変数の変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データマージ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・予測・サブミットファイル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サブミットファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c919f66377084bbe5264acaad3227a0b6e2b7b31c717b6c23ee3b90ae87f6f0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 必要なライブラリのimport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# matplotlibの日本語化対応\n",
    "import japanize_matplotlib\n",
    "\n",
    "# データフレーム表示用関数\n",
    "from IPython.display import display\n",
    "\n",
    "# 表示オプション調整\n",
    "# numpyの浮動小数点の表示精度\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# pandasでの浮動小数点の表示精度\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "# データフレームですべての項目を表示\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "# グラフのデフォルトフォント指定\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "# グラフの表示方法調整\n",
    "sns.set(rc={'figure.figsize':(12,5)});\n",
    "plt.figure(figsize=(12,5));\n",
    "\n",
    "# 乱数の種\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイマー\n",
    "# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習とは\n",
    "機械学習とは、ざっと言ってしまうとあるデータ X を入力として対応する予測値 y を取り出すような対応関係を作成することです。\n",
    "\n",
    "例：タイタニック号で、乗客が生きるか死ぬかを予測する問題だと X は乗客の年齢, 性別, 船室のグレード… など乗客に紐づく情報のことを指します。通常、この情報のことを特徴量とよびます。\n",
    "\n",
    "特徴量 X と 予測値 y が用意できれば学習用データ (X - y の関係がわかっているデータ) を元にして X をいれて y になるようにモデルを調整する。この調整の段階を学習とよびます。学習には様々なアルゴリズムがあるが、X, y を用意しなくてはならない部分は基本的に変わらないです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回のコンペに関して"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンペ自体のデータサイズが大きいのでデータハンドリングが僕たちに取っては肝になりそうです\n",
    "\n",
    "> データの読み込みに関して\n",
    "* 形式を変えたデータセットをpandasで読み込む\n",
    "  * [`feather`](https://www.kaggle.com/datasets/munumbutt/amexfeather) or [`Parquet`](https://www.kaggle.com/datasets/odins0n/amex-parquet)\n",
    "* [`pyspark`](https://www.kaggle.com/code/rakkaalhazimi/export-large-dataset-to-spark) or [`dask`](https://docs.dask.org/en/latest/dataframe.html) で読み込む\n",
    "\n",
    "> サイズが大きい場合の対処方法は以下のkaggle notebookを参考にしてください\n",
    "\n",
    "[How to Work with BIG Datasets on 16G RAM (+Dask)](https://www.kaggle.com/code/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask)\n",
    "\n",
    "以下がnotebookの内容概要\n",
    "* TIP 1 - 使用していない変数を `delete & gc.collect()` で[ガベージコレクション](https://techacademy.jp/magazine/19437)(不要になったメモリ領域を開放して再利用する機能)をする\n",
    "* TIP 2 - データセット内の各カラムのデータタイプを予め定義しておく\n",
    "  * eg: 本来 float64 だったものを -> float32 と定義してサイズを圧縮する\n",
    "* TIP 3 - 読み込むデータセットを選択する (including generating your own subsamples)\n",
    "  * 読み込む行数を選ぶ\n",
    "  * (`skiprows`) で[読み飛ばす行数を指定する](https://bit.ly/3O90Ze7)\n",
    "  * 読み飛ばすリストを作成して、読み飛ばす（以下例）\n",
    "    * ```\n",
    "      skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)\n",
    "      skiplines=np.sort(skiplines)\n",
    "      train = pd.read_csv('../input/train.csv', skiprows=skiplines, dtype=dtypes)\n",
    "      ```\n",
    "* TIP 4 - バッチ処理をする\n",
    "  * ひとまとまりのデータに対して、一連の処理を連続で実行する処理方式のこと。大きなデータに関しても、設定したデータ数(チャンク)ごとに処理をする\n",
    "* TIP 5 - 特定のカラムのみ `import` する\n",
    "  * 450,000行 × 150カラム より 100万行 × 2カラム の方がメモリ消費が少ないことは容易に想像できる\n",
    "* TIP 6 - groupby などの処理をするときも一部カラムの一部データのみにするˆ\n",
    "* TIP 7 - `Dask` を使用する。\n",
    "  * [DaskについてのQiita記事](https://qiita.com/simonritchie/items/e174f243bc03fb25462e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンペの概要\n",
    "* コンペ名：[American Express - Default Prediction](https://www.kaggle.com/competitions/amex-default-prediction)\n",
    "* 目的：毎月の顧客プロファイルから、顧客がクレジットカードの残高分を将来返済しない確率を予測すること\n",
    "  * ターゲットのバイナリ変数は、最新のクレジットカード明細書から18ヶ月間のパフォーマンスウィンドウを観察することによって計算され、顧客が最新の明細書の日付から120日以内に支払額を支払わない場合、デフォルトイベントとみなされます。\n",
    "\n",
    "* データについて\n",
    "  * データセットには各顧客の特徴が各明細書日付ごとに集約されたものが含まれている。特徴は匿名化され、正規化されており、以下のカテゴリに分類されている（カラムの prefix を見ると分かる）:\n",
    "    * D_*: Delinquency variables\n",
    "    * S_*: Spend variables\n",
    "    * P_*: Payment variables\n",
    "    * B_*: Balance variables\n",
    "    * R_*: Risk variables\n",
    "  * 以下のカラムはカテゴリ変数である:\n",
    "    * B_30, B_38, D_63, D_64, D_66, D_68, D_114, D_116, D_117, D_120, D_126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ管理環境整備\n",
    "\n",
    "≒ディレクトリ整備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_dir（input directory） を作ります\n",
    "current_note_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "INPUT_DIR = os.path.join(current_note_path, \"data\")\n",
    "\n",
    "# INPUT_DIRがまだ作られていなければ作成\n",
    "if not os.path.isdir(INPUT_DIR):\n",
    "    os.mkdir(INPUT_DIR)\n",
    "\n",
    "# output_dir(output directory) を作ります\n",
    "OUTPUT_DIR = os.path.join(current_note_path, 'outputs')\n",
    "\n",
    "# OUTPUT_DIRがまだ作られていなければ作成\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/test_data.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/train_labels.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/train_data.csv\n",
      "moved file: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# # csvファイルを `data` ディレクトリ（=フォルダー） に移動させます\n",
    "# unique_dir_names = []\n",
    "# for f in Path(f'{current_note_path}').rglob('*.csv'):\n",
    "#     unique_dir_names.append(f)\n",
    "\n",
    "# for file in list(set(unique_dir_names)):\n",
    "#     print(f'moved file: {file}')\n",
    "#     shutil.move(f'{file}', f'{INPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvを簡単に読み込めるようにする関数\n",
    "# csv を読み取る関数を設定したあげると、pathや拡張子を書かずに読み込めるので入力が楽になります\n",
    "def read_csv(name, **kwrgs):\n",
    "    path = os.path.join(INPUT_DIR, name + '.csv')\n",
    "    print(f'Load: {path}')\n",
    "    return pd.read_csv(path, **kwrgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意点\n",
    "\n",
    "データが30GBとデカイため、クラウド等を使用しない場合（ローカルのcpuのみで頑張る場合）は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/data/test_data.csv\n",
      "Load: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/data/train_data.csv\n",
      "Load: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/data/train_labels.csv\n",
      "Load: /Users/satoshiido/Documents/coding_general/kaggle/amex-default-prediction/data/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_df = read_csv('test_data')\n",
    "train_df = read_csv('train_data')\n",
    "train_labels = read_csv('train_labels')\n",
    "sample_sub = read_csv('sample_submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: object(4), float64(185), int64(1)"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のカラムは頻出で、毎回入力するのはめんどくさいので、ポップアップされるように定義します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各テーブルの概説"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (データ確認)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値・異常値確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理 + 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリ変数の変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データマージ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・予測・サブミットファイル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サブミットファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c919f66377084bbe5264acaad3227a0b6e2b7b31c717b6c23ee3b90ae87f6f0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
